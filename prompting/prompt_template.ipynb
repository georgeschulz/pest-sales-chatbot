{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "from langchain.prompts import load_prompt\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import OpenAIChat\n",
    "import sys\n",
    "pd.options.mode.chained_assignment = None\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "import sys\n",
    "sys.path.append('./modules/')\n",
    "openai.api_key = os.getenv('OPENAI_KEY')\n",
    "key = os.getenv('OPENAI_KEY')\n",
    "\n",
    "from Generator import Generator\n",
    "from Extractor import Extractor\n",
    "from PromptPath import PromptPath\n",
    "from GenExtract import GenExtract\n",
    "from Property import Property\n",
    "from NaturalLanguagePool import NaturalLanguagePool\n",
    "from DynamicChatHistory import DynamicChatHistory\n",
    "from Conversation import Conversation\n",
    "from ConversationScript import ConversationScript\n",
    "from ConversationScriptStage import ConversationScriptStage\n",
    "from show_chat_log import show_chat_log\n",
    "from is_question import is_question\n",
    "from customer_profile import customer_profile\n",
    "\n",
    "times_embedding_run = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Create a directory in the prompts folder to store the contents. Then save the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt folder exists\n"
     ]
    }
   ],
   "source": [
    "#prompt folder_name\n",
    "prompt_folder_name = 'customer_issues_extractor'\n",
    "\n",
    "#check to see if the the folder name exists in the prompts folder\n",
    "if prompt_folder_name in os.listdir('prompts'):\n",
    "    print('prompt folder exists')\n",
    "    prompt_folder_path = 'prompts/' + prompt_folder_name\n",
    "else:\n",
    "    os.mkdir('prompts/' + prompt_folder_name)\n",
    "    prompt_folder_path = 'prompts/' + prompt_folder_name\n",
    "\n",
    "path = PromptPath(prompt_folder_path + '/', type=\"json\")\n",
    "\n",
    "def save(prompt, name):\n",
    "    prompt.save(prompt_folder_path + '/' + name + '.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Create your base prompt & Test It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Testing prompt: {customer_response}. Your response:\"\"\"\n",
    "prompt = PromptTemplate(template=text, input_variables=[\"customer_response\"])\n",
    "\n",
    "save(prompt, 'base_prompt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Create the first version of the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenExtract(\n",
    "    Generator(path('base_prompt')),\n",
    "    Extractor(regex=r\"(True|False)\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No Classification Found'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.run('Your first message here')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Switch out the generator and prompt using the following code format\n",
    "\n",
    "`model.generator = Generator(path('new_prompt'))`\n",
    "\n",
    "`model.run('Test Case')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec0f111f5e5040c678ec5a567f7682a80a3d7b8519cf7f13b3475890ed8a264f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
